{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Задача: запустить модель LDA и Gibbs Sampling с числов тегов 20. Вывести топ-10 слов по каждому тегу. Соотнести полученные теги с тегами из датасета. Добейтесь того, чтобы хотя бы несколько тем были явно интерпретируемы, например, как в примерах ниже.\n",
        "\n",
        "Примеры топ-10 слов из некотрых тегов, которые получаются после применения LDA:\n",
        "* ['god', 'jesus', 'believe', 'life', 'bible', 'christian', 'world', 'church', 'word', 'people'] - эта группа явно соотносится с soc.religion.christian\n",
        "* ['drive', 'card', 'hard', 'bit', 'disk', 'scsi', 'memory', 'speed', 'mac', 'video'] - эту группу можно соотнести с темами 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware'\n",
        "* ['game',\t'games',\t'hockey',\t'league',\t'play',\t'players',\t'season',\t'team',\t'teams',\t'win'] - тема rec.sport.hockey\n",
        "\n",
        "Советы:\n",
        "* модель будет сходится лучше и быстрее, если уменьшить размер словаря за счет отсеивания общеупотребительных слов и редких слов. Управлять размером словаря можно с помощью параметров min_df (отсеивает слова по минимальной частоте встречаемости) и max_df (отсеивает слова по максимальной частоте встречаемости) в CountVectorizer.\n",
        "* параметры $\\alpha$, $\\beta$ можно, для начала, положить единицами\n",
        "* после 100 итераций можно ожидать хорошего распределения по темам. Если этого не происходит и в темах мешинина - проверяйте код и оптимизируйте словарь\n",
        "* на примере третьей темы видно, что у нас встречаются разные формы одного и того же слова. С помощью процедур stemming и lemmatization можно привести слова к общей форме и объединить близкие по значению"
      ],
      "metadata": {
        "id": "0LhXECNbyfIk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XalAeq0VvcIy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words=\"english\", analyzer='word', binary=True, min_df = 0.0001, max_df = 0.001)\n",
        "vectorizer.fit(newsgroups_train.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfhKkijlwclW",
        "outputId": "9587f334-3582-4cf6-92a2-420b62bbcefc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(binary=True, max_df=0.001, min_df=0.0001, stop_words='english')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArC09dEcyVWE",
        "outputId": "3e9cbbce-b879-4662-912d-a0870bca50f6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 30063)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u = X_train.toarray()"
      ],
      "metadata": {
        "id": "ljfWr1Xszu22"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.vocabulary_.get(25717))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Twm6XG-kJvM",
        "outputId": "ebe788fe-39ec-4c47-c4a6-e4a25fba2f9f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "С этого места можно полностью собрать алгоритм моделирования плотности $p(\\textbf{z}| \\textbf{w}, \\alpha, \\beta)$. Введем  обозначения \n",
        "* $n_k$ - количество слов, отнесенных к тегу $k$, по всем документам\n",
        "* $n_{k, w}$ - количество раз, когда слово $w$ вошло в тег $k$\n",
        "* $n_{d, k}$ - количество вхождений тега $k$ в документ $d$\n",
        "* $W$ - общее количество слов в корпусе документов\n",
        "* $\\beta_{sum} = \\sum\\limits_{w=1}^{N}\\beta_w$"
      ],
      "metadata": {
        "id": "r9tI3o2tQL1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* заведем счетчики $n_{k, w}$, $n_{d, k}$, $n_k$, заполненные нулями"
      ],
      "metadata": {
        "id": "fnTe_2MYQuNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tags = 20\n",
        "n_k = np.zeros(tags)\n",
        "n_k_w = np.zeros(tags * X_train.shape[1]).reshape(tags,X_train.shape[1])\n",
        "n_d_k = np.zeros(tags * X_train.shape[0]).reshape(X_train.shape[0],tags)"
      ],
      "metadata": {
        "id": "XJfnOcXH0dGU"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* случайным образом расставим теги словам, обновим счетчики $n_{k, w}$, $n_{d, k}$, $n_k$\n"
      ],
      "metadata": {
        "id": "xH5-iE3SRckJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dc, word = X_train.nonzero()\n",
        "z = np.random.choice(tags, len(dc))\n",
        "\n",
        "for i, j, k in zip(dc, word, z):\n",
        "    n_k[k] += 1\n",
        "    n_k_w[k, j] += 1\n",
        "    n_d_k[i, k] += 1"
      ],
      "metadata": {
        "id": "Lk3EzVm6QEiJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* пока не сойдемся к стационарному режиму:\n",
        "  * для каждого $i$ от 1 до $W$:\n",
        "      * $n_{d_i, z_i} \\mathrel{-}= 1$, $n_{z_i, w_i} \\mathrel{-}= 1$, $n_{z_i} \\mathrel{-}= 1$\n",
        "      * для каждого $k$ от 1 до $K$:\n",
        "        * вычисляем $p_k = (n_{d, k} + \\alpha_k) \\frac{n_{k, w_i} + \\beta_{w_i}}{n_k + \\beta_{sum}}$\n",
        "      * сэмплим новый $z_i$ из полученного распределения $(p_1, ..., p_K)$\n",
        "      * $n_{d_i, z_i} \\mathrel{+}= 1$, $n_{z_i, w_i} \\mathrel{+}= 1$, $n_{z_i} \\mathrel{+}= 1$"
      ],
      "metadata": {
        "id": "B8VyjW4bSRFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LDA(n_d_k, n_k_w, n_k, z, dc, word, tags, alpha, betta, it):    \n",
        "    for i in range(it):\n",
        "        for j in range(len(dc)):\n",
        "            cword = word[j]\n",
        "            cdc = dc[j]\n",
        "            corder = z[j]\n",
        "\n",
        "            n_k[corder] -= 1\n",
        "            n_k_w[corder, cword] -= 1\n",
        "            n_d_k[cdc, corder] -= 1\n",
        "\n",
        "            p = (n_d_k[cdc, :] + alpha) * (n_k_w[:, cword] + betta[cword]) / (n_k + betta.sum())\n",
        "            \n",
        "            z[j] = np.random.choice(np.arange(tags), p = p / p.sum())\n",
        "            \n",
        "            n_k[corder] += 1\n",
        "            n_k_w[corder, cword] += 1\n",
        "            n_d_k[cdc, corder] += 1\n",
        "    return z, n_k_w, n_d_k, n_k"
      ],
      "metadata": {
        "id": "3rVJuymBSXRp"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* зададим априорные $\\beta$, $\\alpha$"
      ],
      "metadata": {
        "id": "zgAKCm_jTes5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = np.ones(20)\n",
        "betta = np.ones(X_train.shape[1])"
      ],
      "metadata": {
        "id": "8_ItPypFTd4s"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Основная часть алгоритма:"
      ],
      "metadata": {
        "id": "YCsn_N8DT6GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z ,n_k_w, n_d_k, n_k = LDA(n_d_k, n_k_w, n_k, z, dc, word, 20, alpha, betta, 100)"
      ],
      "metadata": {
        "id": "HBZflLXPTlQl"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Печатаем топ значений"
      ],
      "metadata": {
        "id": "74YQfqOOT-tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = np.argsort(n_k_w, axis=1)[:, -10:]\n",
        "for i in range(20):\n",
        "    matrix = np.zeros((1, X_train.shape[1]))\n",
        "    for j in result[i]:\n",
        "        matrix[0, j] = 1\n",
        "    print('\\nTAG  {}:   {}'.format(i+1, '\\t'.join(vectorizer.inverse_transform(matrix)[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO6jqWKxUBV5",
        "outputId": "02495817-369c-4f60-abf2-10495537bfa6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TAG  1:   batman\tcompressor\tparliamentary\tquartz\trespecting\tslam\tspacelink\tsupervisor\tw0\tx7\n",
            "\n",
            "TAG  2:   1922\t1949\taspirations\tccwf\tforbids\tkovalev\tlag\tneurons\tpopulated\tspray\n",
            "\n",
            "TAG  3:   32k\tattackers\tb8e\tcatalogs\tcoin\tfla\tiifx\tirresponsible\tpersists\tunicorns\n",
            "\n",
            "TAG  4:   boulevard\tcats\tceased\tdating\tderivative\temphasizes\tencounters\traids\tspies\tsuperstar\n",
            "\n",
            "TAG  5:   0x\t6j\tcircumference\tdiscrepancy\texcommunicated\tintolerance\tqgq\treichel\tsimtel20\tworded\n",
            "\n",
            "TAG  6:   0w\t2tg\tbelievable\tcalmly\tenclosed\tequalizer\tfinancially\tgrandmother\tingredients\tmodifying\n",
            "\n",
            "TAG  7:   airplanes\tcrooks\tdecree\tdutch\tfahrenheit\tgainey\tgraduation\tmassage\tpayne\twingers\n",
            "\n",
            "TAG  8:   1980s\tbeastmaster\tfascism\theavier\thiking\tmelt\tphigs\tprescription\ttennessee\tund\n",
            "\n",
            "TAG  9:   authoritarian\tbleeding\tbureaucracy\tcancel\teditions\tips\tmonkey\trpw\trub\tsurgeons\n",
            "\n",
            "TAG  10:   955\tcitizenry\tdevotion\temployers\tinduction\tnoring\tpopping\tsecuring\tteens\twireframe\n",
            "\n",
            "TAG  11:   9m\tdrawbacks\tentryway\tfw\tg6\tinternals\tminimally\tpsi\ty4\tyah\n",
            "\n",
            "TAG  12:   295\tbackdoor\telastic\tinsertion\tobjectivity\tpasteur\tperpendicular\tpractitioner\tprizes\ttoner\n",
            "\n",
            "TAG  13:   615\tastray\tcurse\tdude\tenhancements\tjoysticks\tjuan\tjudicial\tkmail\tt6\n",
            "\n",
            "TAG  14:   bullard\tbuster\tgreenbelt\timaginary\tlent\tnancy\tneglected\tphy\ttabs\tz1\n",
            "\n",
            "TAG  15:   7_\talliance\tbhjn\tc7\tconventions\tcorresponds\tneedles\torgans\tplots\ttrashing\n",
            "\n",
            "TAG  16:   632\tequality\tgenocidal\tkindly\tnyr\trepaired\tsettlers\tuj\twiretapping\txmodmap\n",
            "\n",
            "TAG  17:   494\tagony\tb9\tbroadcasting\tfairing\thedican\tpumped\tteflon\tterrestrial\tyields\n",
            "\n",
            "TAG  18:   474\tadditives\tattorneys\teo\thurting\tinference\tpep\trecomend\tstrengthen\tz7\n",
            "\n",
            "TAG  19:   535\tcastle\tevasion\tfatah\tgnv\tkp\tmarker\tmerge\tobfuscate\tredirect\n",
            "\n",
            "TAG  20:   455\tbuddhist\tdish\tfoam\titar\tpadding\tpsych\trushdie\tsail\tslipped\n"
          ]
        }
      ]
    }
  ]
}